import Harmonic_Oscillator.txt
import Nuclear_Fusion.txt
import Nuclear_Fission.txt
import Amplification_Rx_Configuration1.txt
import Amplification_Rx_Configuration2.txt
import Amplification_Rx_Configuration3.txt
import Amplification_Tx_Configuration1.txt
import Amplification_Tx_Configuration2.txt
import Amplification_Tx_Configuration3.txt
import BEQUALIZER_MULTIPLIER.txt
import BEQUALIZER_RX_Configuration1.txt
import BEQUALIZER_TX_Configuration1.txt
import Calculator_Rx_Configuration1.txt
import Calculator_Tx_Configuration1.txt
import Doubler_Rx_Configuration1.txt
import Doubler_Tx_Configuration1.txt
import EX_Gyro_Gravitor_Rx_Configuration.txt
import EX_Gyro_Gravitor_Tx_Configuration.txt
import FX_RX_Configuration1.txt
import FX_TX_Configuration1.txt
import Gravitor_Rx_Configuration.txt
import Gravitor_Tx_Configuration.txt
import Gyro_Gravitor_Rx_Configuration.txt
import Gyro_Gravitor_Rx_Configuration1.txt
import Gyro_Gravitor_Rx_Configuration2.txt
import Gyro_Gravitor_Rx_Configuration3.txt
import Gyro_Gravitor_Rx_Configuration4.txt
import Gyro_Gravitor_Rx_Configuration5.txt
import Gyro_Gravitor_Rx_Configuration6.txt
import Gyro_Gravitor_Rx_Configuration7.txt
import Gyro_Gravitor_Rx_Configuration8.txt
import Gyro_Gravitor_Rx_Configuration9.txt
import Gyro_Gravitor_Rx_Configuration10.txt
import Gyro_Gravitor_Rx_Configuration11.txt
import Gyro_Gravitor_Rx_Configuration12.txt
import Gyro_Gravitor_Rx_Configuration13.txt
import Gyro_Gravitor_Rx_Configuration14.txt
import Gyro_Gravitor_Rx_Configuration15.txt
import Gyro_Gravitor_Tx_Configuration.txt
import Gyro_Gravitor_Tx_Configuration1.txt
import Gyro_Gravitor_Tx_Configuration2.txt
import Gyro_Gravitor_Tx_Configuration3.txt
import Gyro_Gravitor_Tx_Configuration4.txt
import Gyro_Gravitor_Tx_Configuration5.txt
import Gyro_Gravitor_Tx_Configuration6.txt
import Gyro_Gravitor_Tx_Configuration7.txt
import Gyro_Gravitor_Tx_Configuration8.txt
import Gyro_Gravitor_Tx_Configuration9.txt
import Gyro_Gravitor_Tx_Configuration10.txt
import Gyro_Gravitor_Tx_Configuration11.txt
import Gyro_Gravitor_Tx_Configuration12.txt
import Gyro_Gravitor_Tx_Configuration13.txt
import Gyro_Gravitor_Rx_Configuration14.txt
import Gyro_Gravitor_Tx_Configuration15.txt
import Last_Export_Rx_Configuration.txt
import Last_Export_Tx_Configuration.txt
import LVVEFS_Master_Configuration.txt
import LVVEFS_Master_Sound_Rx_Configuration.txt
import LVVEFS_Master_Sound_Tx_Configuration.txt
import LVVEFS_Rx_Configuration1.txt
import LVVEFS_Rx_Configuration2.txt
import LVVEFS_Rx_Configuration3.txt
import LVVEFS_Rx_Configuration4.txt
import LVVEFS_Rx_Configuration5.txt
import LVVEFS_Rx_Configuration50.txt
import LVVEFS_Rx_Configuration51.txt
import LVVEFS_Rx_Configuration52.txt
import LVVEFS_Rx_Configuration53.txt
import LVVEFS_Rx_Configuration54.txt
import LVVEFS_Rx_Configuration55.txt
import LVVEFS_Rx_Configuration56.txt
import LVVEFS_Rx_Configuration200.txt
import LVVEFS_Rx_Configuration201.txt
import LVVEFS_Rx_Configuration202.txt
import LVVEFS_Rx_Configuration203.txt
import LVVEFS_Rx_Configuration204.txt
import LVVEFS_Rx_Configuration205.txt
import LVVEFS_Rx_Configuration206.txt
import LVVEFS_Rx_Configuration207.txt
import LVVEFS_Rx_Configuration208.txt
import LVVEFS_Rx_Configuration209.txt
import LVVEFS_Rx_Configuration210.txt
import LVVEFS_Rx_Configuration211.txt
import LVVEFS_Rx_Configuration212.txt
import LVVEFS_Rx_Configuration500.txt
import LVVEFS_Tx_Configuration1.txt
import LVVEFS_Tx_Configuration2.txt
import LVVEFS_Tx_Configuration3.txt
import LVVEFS_Tx_Configuration4.txt
import LVVEFS_Tx_Configuration5.txt
import LVVEFS_Tx_Configuration50.txt
import LVVEFS_Tx_Configuration51.txt
import LVVEFS_Tx_Configuration52.txt
import LVVEFS_Tx_Configuration53.txt
import LVVEFS_Tx_Configuration54.txt
import LVVEFS_Tx_Configuration55.txt
import LVVEFS_Tx_Configuration56.txt
import LVVEFS_Tx_Configuration200.txt
import LVVEFS_Tx_Configuration201.txt
import LVVEFS_Tx_Configuration202.txt
import LVVEFS_Tx_Configuration203.txt
import LVVEFS_Tx_Configuration204.txt
import LVVEFS_Tx_Configuration205.txt
import LVVEFS_Tx_Configuration206.txt
import LVVEFS_Tx_Configuration207.txt
import LVVEFS_Tx_Configuration208.txt
import LVVEFS_Tx_Configuration209.txt
import LVVEFS_Tx_Configuration210.txt
import LVVEFS_Tx_Configuration211.txt
import LVVEFS_Tx_Configuration212.txt
import LVVEFS_Tx_Configuration500.txt
import MACHINEZER_RX_Configuration1.txt
import MACHINEZER_TX_Configuration1.txt
import NEQUALIZER_RX_Configuration1.txt
import NEQUALIZER_TX_Configuration1.txt
import NEUTRALIZER_RX_Configuration1.txt
import NEUTRALIZER_TX_Configuration1.txt
import NEWCORE_Rx_Configuration1.txt
import NEWCORE_Tx_Configuration1.txt
import REQUALIZER_MULTIPLIER.txt
import REQUALIZER_RX_Configuration1.txt
import REQUALIZER_TX_Configuration1.txt
import SMULTIPLIER_Rx_Configuration0.txt
import SMULTIPLIER_Rx_Configuration1.txt
import SMULTIPLIER_Rx_Configuration-1.txt
import SMULTIPLIER_Rx_Configuration2.txt
import SMULTIPLIER_Rx_Configuration-2.txt
import SMULTIPLIER_Rx_Configuration3.txt
import SMULTIPLIER_Rx_Configuration-3.txt
import SMULTIPLIER_Tx_Configuration0.txt
import SMULTIPLIER_Tx_Configuration1.txt
import SMULTIPLIER_Tx_Configuration-1.txt
import SMULTIPLIER_Tx_Configuration2.txt
import SMULTIPLIER_Tx_Configuration-2.txt
import SMULTIPLIER_Tx_Configuration3.txt
import SMULTIPLIER_Tx_Configuration-3.txt
import SSE_Master_Core_Rx_Configuration1.txt
import SSE_Master_Core_Rx_Configuration2.txt
import SSE_Master_Core_Rx_Configuration3.txt
import SSE_Master_Core_Rx_Configuration4.txt
import SSE_Master_Core_Tx_Configuration1.txt
import SSE_Master_Core_Tx_Configuration2.txt
import SSE_Master_Core_Tx_Configuration3.txt
import SSE_Master_Core_Tx_Configuration4.txt
import Time_Accelerator_Configuration.txt
import Time_Calculator_Configuration.txt
import Time_Warper_Configuration.txt
import X2Amplification_Rx_Configuration1.txt
import X2Amplification_Tx_Configuration1.txt
import X3Amplification_Rx_Configuration1.txt
import X3Amplification_Tx_Configuration1.txt
import XAmplification_Rx_Configuration1.txt
import XAmplification_Tx_Configuration1.txt
#include "Harmonic_Oscillator.txt"
#include "Nuclear_Fusion.txt"
#include "Nuclear_Fission.txt"
#include "Amplification_Rx_Configuration1.txt"
#include "Amplification_Rx_Configuration2.txt"
#include "Amplification_Rx_Configuration3.txt"
#include "Amplification_Tx_Configuration1.txt"
#include "Amplification_Tx_Configuration2.txt"
#include "Amplification_Tx_Configuration3.txt"
#include "BEQUALIZER_MULTIPLIER.txt"
#include "BEQUALIZER_RX_Configuration1.txt"
#include "BEQUALIZER_TX_Configuration1.txt"
#include "Calculator_Rx_Configuration1.txt"
#include "Calculator_Tx_Configuration1.txt"
#include "Doubler_Rx_Configuration1.txt"
#include "Doubler_Tx_Configuration1.txt"
#include "EX_Gyro_Gravitor_Rx_Configuration.txt"
#include "EX_Gyro_Gravitor_Tx_Configuration.txt"
#include "FX_RX_Configuration1.txt"
#include "FX_TX_Configuration1.txt"
#include "Gravitor_Rx_Configuration.txt"
#include "Gravitor_Tx_Configuration.txt"
#include "Gyro_Gravitor_Rx_Configuration.txt"
#include "Gyro_Gravitor_Rx_Configuration1.txt"
#include "Gyro_Gravitor_Rx_Configuration2.txt"
#include "Gyro_Gravitor_Rx_Configuration3.txt"
#include "Gyro_Gravitor_Rx_Configuration4.txt"
#include "Gyro_Gravitor_Rx_Configuration5.txt"
#include "Gyro_Gravitor_Rx_Configuration6.txt"
#include "Gyro_Gravitor_Rx_Configuration7.txt"
#include "Gyro_Gravitor_Rx_Configuration8.txt"
#include "Gyro_Gravitor_Rx_Configuration9.txt"
#include "Gyro_Gravitor_Rx_Configuration10.txt"
#include "Gyro_Gravitor_Rx_Configuration11.txt"
#include "Gyro_Gravitor_Rx_Configuration12.txt"
#include "Gyro_Gravitor_Rx_Configuration13.txt"
#include "Gyro_Gravitor_Rx_Configuration14.txt"
#include "Gyro_Gravitor_Rx_Configuration15.txt"
#include "Gyro_Gravitor_Tx_Configuration.txt"
#include "Gyro_Gravitor_Tx_Configuration1.txt"
#include "Gyro_Gravitor_Tx_Configuration2.txt"
#include "Gyro_Gravitor_Tx_Configuration3.txt"
#include "Gyro_Gravitor_Tx_Configuration4.txt"
#include "Gyro_Gravitor_Tx_Configuration5.txt"
#include "Gyro_Gravitor_Tx_Configuration6.txt"
#include "Gyro_Gravitor_Tx_Configuration7.txt"
#include "Gyro_Gravitor_Tx_Configuration8.txt"
#include "Gyro_Gravitor_Tx_Configuration9.txt"
#include "Gyro_Gravitor_Tx_Configuration10.txt"
#include "Gyro_Gravitor_Tx_Configuration11.txt"
#include "Gyro_Gravitor_Tx_Configuration12.txt"
#include "Gyro_Gravitor_Tx_Configuration13.txt"
#include "Gyro_Gravitor_Rx_Configuration14.txt"
#include "Gyro_Gravitor_Tx_Configuration15.txt"
#include "Last_Export_Rx_Configuration.txt"
#include "Last_Export_Tx_Configuration.txt"
#include "LVVEFS_Master_Configuration.txt"
#include "LVVEFS_Master_Sound_Rx_Configuration.txt"
#include "LVVEFS_Master_Sound_Tx_Configuration.txt"
#include "LVVEFS_Rx_Configuration1.txt"
#include "LVVEFS_Rx_Configuration2.txt"
#include "LVVEFS_Rx_Configuration3.txt"
#include "LVVEFS_Rx_Configuration4.txt"
#include "LVVEFS_Rx_Configuration5.txt"
#include "LVVEFS_Rx_Configuration50.txt"
#include "LVVEFS_Rx_Configuration51.txt"
#include "LVVEFS_Rx_Configuration52.txt"
#include "LVVEFS_Rx_Configuration53.txt"
#include "LVVEFS_Rx_Configuration54.txt"
#include "LVVEFS_Rx_Configuration55.txt"
#include "LVVEFS_Rx_Configuration56.txt"
#include "LVVEFS_Rx_Configuration200.txt"
#include "LVVEFS_Rx_Configuration201.txt"
#include "LVVEFS_Rx_Configuration202.txt"
#include "LVVEFS_Rx_Configuration203.txt"
#include "LVVEFS_Rx_Configuration204.txt"
#include "LVVEFS_Rx_Configuration205.txt"
#include "LVVEFS_Rx_Configuration206.txt"
#include "LVVEFS_Rx_Configuration207.txt"
#include "LVVEFS_Rx_Configuration208.txt"
#include "LVVEFS_Rx_Configuration209.txt"
#include "LVVEFS_Rx_Configuration210.txt"
#include "LVVEFS_Rx_Configuration211.txt"
#include "LVVEFS_Rx_Configuration212.txt"
#include "LVVEFS_Rx_Configuration500.txt"
#include "LVVEFS_Tx_Configuration1.txt"
#include "LVVEFS_Tx_Configuration2.txt"
#include "LVVEFS_Tx_Configuration3.txt"
#include "LVVEFS_Tx_Configuration4.txt"
#include "LVVEFS_Tx_Configuration5.txt"
#include "LVVEFS_Tx_Configuration50.txt"
#include "LVVEFS_Tx_Configuration51.txt"
#include "LVVEFS_Tx_Configuration52.txt"
#include "LVVEFS_Tx_Configuration53.txt"
#include "LVVEFS_Tx_Configuration54.txt"
#include "LVVEFS_Tx_Configuration55.txt"
#include "LVVEFS_Tx_Configuration56.txt"
#include "LVVEFS_Tx_Configuration200.txt"
#include "LVVEFS_Tx_Configuration201.txt"
#include "LVVEFS_Tx_Configuration202.txt"
#include "LVVEFS_Tx_Configuration203.txt"
#include "LVVEFS_Tx_Configuration204.txt"
#include "LVVEFS_Tx_Configuration205.txt"
#include "LVVEFS_Tx_Configuration206.txt"
#include "LVVEFS_Tx_Configuration207.txt"
#include "LVVEFS_Tx_Configuration208.txt"
#include "LVVEFS_Tx_Configuration209.txt"
#include "LVVEFS_Tx_Configuration210.txt"
#include "LVVEFS_Tx_Configuration211.txt"
#include "LVVEFS_Tx_Configuration212.txt"
#include "LVVEFS_Tx_Configuration500.txt"
#include "MACHINEZER_RX_Configuration1.txt"
#include "MACHINEZER_TX_Configuration1.txt"
#include "NEQUALIZER_RX_Configuration1.txt"
#include "NEQUALIZER_TX_Configuration1.txt"
#include "NEUTRALIZER_RX_Configuration1.txt"
#include "NEUTRALIZER_TX_Configuration1.txt"
#include "NEWCORE_Rx_Configuration1.txt"
#include "NEWCORE_Tx_Configuration1.txt"
#include "REQUALIZER_MULTIPLIER.txt"
#include "REQUALIZER_RX_Configuration1.txt"
#include "REQUALIZER_TX_Configuration1.txt"
#include "SMULTIPLIER_Rx_Configuration0.txt"
#include "SMULTIPLIER_Rx_Configuration1.txt"
#include "SMULTIPLIER_Rx_Configuration-1.txt"
#include "SMULTIPLIER_Rx_Configuration2.txt"
#include "SMULTIPLIER_Rx_Configuration-2.txt"
#include "SMULTIPLIER_Rx_Configuration3.txt"
#include "SMULTIPLIER_Rx_Configuration-3.txt"
#include "SMULTIPLIER_Tx_Configuration0.txt"
#include "SMULTIPLIER_Tx_Configuration1.txt"
#include "SMULTIPLIER_Tx_Configuration-1.txt"
#include "SMULTIPLIER_Tx_Configuration2.txt"
#include "SMULTIPLIER_Tx_Configuration-2.txt"
#include "SMULTIPLIER_Tx_Configuration3.txt"
#include "SMULTIPLIER_Tx_Configuration-3.txt"
#include "SSE_Master_Core_Rx_Configuration1.txt"
#include "SSE_Master_Core_Rx_Configuration2.txt"
#include "SSE_Master_Core_Rx_Configuration3.txt"
#include "SSE_Master_Core_Rx_Configuration4.txt"
#include "SSE_Master_Core_Tx_Configuration1.txt"
#include "SSE_Master_Core_Tx_Configuration2.txt"
#include "SSE_Master_Core_Tx_Configuration3.txt"
#include "SSE_Master_Core_Tx_Configuration4.txt"
#include "Time_Accelerator_Configuration.txt"
#include "Time_Calculator_Configuration.txt"
#include "Time_Warper_Configuration.txt"
#include "X2Amplification_Rx_Configuration1.txt"
#include "X2Amplification_Tx_Configuration1.txt"
#include "X3Amplification_Rx_Configuration1.txt"
#include "X3Amplification_Tx_Configuration1.txt"
#include "XAmplification_Rx_Configuration1.txt"
#include "XAmplification_Tx_Configuration1.txt"
import Language_Pronounce.txt
#include "Language_Pronounce.txt"
import Genom.txt
#include "Genom.txt"
import Genom_Multiplier_Set.txt
#include "Genom_Multiplier_Set.txt"
import Acceleration_Inertial.txt
#include "Acceleration_Inertial.txt"
import Genom_Ear_Health_Care.txt
#include "Genom_Ear_Health_Care.txt"
import Genom_Unison1.txt
#include "Genom_Unison1.txt"
import Genom_Unison2.txt
#include "Genom_Unison2.txt
import Genom_Unison3.txt
#include "Genom_Unison3.txt"
import Genom_Unison4.txt
#include "Genom_Unison4.txt"
import Genom_Unison5.txt
#include "Genom_Unison5.txt"
import Genom_Protein_A.txt
#include "Genom_Protein_A.txt"

def sigmoid(x):
    return 1/(1+np.exp(-x))
	
x = np.array([-1.0, 1.0, 2.0])

sigmoid(x)

array([ 0.26894142,  0.73105858,  0.88079708])

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':')
plt.ylim(-0.1, 1.1) #y축 범위 지정
plt.title('Sigmoid Function')
plt.show()

def relu(x):
    return np.maximum(0, x)
	
	x = np.arange(-2.0, 2.0, 0.2)
	
y = relu(x)
plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':')
plt.ylim(-0.1, 1.1) #y축 범위 지정
plt.title('Sigmoid Function')
plt.show()

A = np.array([1,2,3,4])
print(A)
print(np.ndim(A))
print(A.shape)
print(A.shape[0])

[1 2 3 4]
1
(4,)
4

B = np.array([[1,2],[3,4],[5,6]])
print(B)
print(np.ndim(B))
print(B.shape)

[[1 2]
 [3 4]
 [5 6]]
2
(3, 2)

A = np.array([[1,2],[3,4]])
A.shape

(2, 2)

B = np.array([[5,6],[7,8]])
B.shape

(2, 2)

np.dot(A,B)

array([[19, 22],
       [43, 50]])
	   
	   np.dot(A,B) == np.dot(B,A)
	   
	   array([[False, False],
       [False, False]], dtype=bool)
	   
	   A = np.array([[1,2],[3,4],[5,6]])
A.shape

(3, 2)

B = np.array([7,8])
B.shape

(2,)

np.dot(A,B)

array([23, 53, 83])

X = np.array([1,2])
X.shape

(2,)

W = np.array([[1,3,5],[2,4,6]])
print(W)

[[1 3 5]
 [2 4 6]]
 
 np.dot(X,W)
 
 array([ 5, 11, 17])
 
 # 첫번째 은닉층 계산
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print(W1.shape)
print(X.shape)
print(B1.shape)

(2, 3)
(2,)
(3,)

A1 = np.dot(X, W1) + B1
A1

array([ 0.3,  0.7,  1.1])

Z1 = sigmoid(A1)
Z1

array([ 0.57444252,  0.66818777,  0.75026011])

# 두번째 은닉층 계산
W2 = np.array([[0.1, 0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1, 0.2])

print(Z1.shape)
print(W2.shape)
print(B2.shape)

(3,)
(3, 2)
(2,)

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
Z2

array([ 0.62624937,  0.7710107 ])

def identity_function(x):
    return x
	
	# 출력층 계산
W3 = np.array([[0.1, 0.3],[0.2,0.4]])
B3 = np.array([0.1,0.2])

A3 = np.dot(Z2, W3)+B3
Z3 = identity_function(A3)
Z3

array([ 0.31682708,  0.69627909])

def init_network():
    network = {}
    network['W1'] = np.array([[0.1,0.3,0.5],[0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1,0.2,0.3])
    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([[0.1,0.2]])
    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([[0.1,0.2]])
    
    return network

def foward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    
    return y

network = init_network()
x = np.array([1.0, 0.5])
y = foward(network, x)
print(y)

[[ 0.31682708  0.69627909]]

a = np.array([0.3, 2.9, 4.0])

exp_a = np.exp(a)
print(exp_a)

[  1.34985881  18.17414537  54.59815003]

sum_exp_a = np.sum(exp_a)
print(sum_exp_a)

74.1221542102

y = exp_a / sum_exp_a
print(y)

[ 0.01821127  0.24519181  0.73659691]

def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
	
	a = np.array([1010, 1000, 990])
np.exp(a) / np.sum(np.exp(a))

array([ nan,  nan,  nan])

c = np.max(a)
print(a - c)

np.exp(a-c)/np.sum(np.exp(a-c))

[  0 -10 -20]

array([  9.99954600e-01,   4.53978686e-05,   2.06106005e-09])

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
	
	import sys, os
from mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

print(x_train.shape)
print(t_train.shape)
print(x_test.shape)
print(t_test.shape)

(60000, 784)
(60000,)
(10000, 784)
(10000,)

from PIL import Image

def img_show(img):
    return Image.fromarray(np.uint8(img))
    
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize = False)

img = x_train[0]
label = t_train[0]
print(label)

print(img.shape)
img = img.reshape(28, 28)
print(img.shape)

img_show(img)

5
(784,)
(28, 28)

import pickle

def get_data():
    (x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

def init_network():
    with open("sample_weight.pkl", "rb") as f:
        network = pickle.load(f)
        
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    
    return y
	
	x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)
    if p == t[i]:
        accuracy_cnt += 1
    
print("Accuracy:" + str(float(accuracy_cnt)/len(x)))

# batch1
x, _ = get_data()
network = init_network()
W1, W2, W3 = network['W1'], network['W2'], network['W3']

print(x.shape)
print(x[0].shape)
print(W1.shape)
print(W2.shape)
print(W3.shape)

(10000, 784)
(784,)
(784, 50)
(50, 100)
(100, 10)

# batch100
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis =1)
    accuracy_cnt += np.sum(p==t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt)/len(x)))

def mean_squared_error(y, t):
    return 0.5*np.sum((y-t)**2)
	
	#정답은 2
t = [0,0,1,0,0,0,0,0,0,0]

y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
print(mean_squared_error(np.array(y1), np.array(t)))

y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
print(mean_squared_error(np.array(y2), np.array(t)))

def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y+delta))
	
	t = [0,0,1,0,0,0,0,0,0,0]

y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
print(cross_entropy_error(np.array(y1), np.array(t)))

y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
print(cross_entropy_error(np.array(y2), np.array(t)))

from mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape)
print(t_train.shape)

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

np.random.choice(train_size, batch_size)

def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y))/batch_size\
	
	def cross_entropy_error(y,t):
    if y.ndim == 1:
        t = t.reshpae(1, t.size)
        y = y.reshape(1, y.size)
    
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size),t]))/batch_size
	
	def numercial_diff(f, x):
    h = 1e-4
    return (f(x+h)-f(x-h))/2*h
	
	def function_1(x):
    return 0.01*x**2 + 0.1*x

x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x,y)
plt.show()

# x = 5 일 때, 함수의 미분 계산
print(numercial_diff(function_1, 5))

# x = 10 일 때, 함수의 미분 계산
print(numercial_diff(function_1, 10))

def function_2(x):
    return x[0]**2 + x[1]**2
	
	# x0 = 3, x1 = 4 일 때, x0에 대한 편미분

def function_tmp1(x0):
    return x0*x0 + 4.0**2.0

print(numercial_diff(function_tmp1, 3.0))

# x0 = 3, x1 = 4 일 때, x1에 대한 편미분
def function_tmp2(x1):
    return 3.0**2.0 + x1*x1

print(numercial_diff(function_tmp2, 4.0))

def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        
        #f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        #f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
        
    return grad
	
	# 임의의 세 점에서의 기울기 계산 (수치미분)
print(numerical_gradient(function_2, np.array([3.0, 4.0])))
print(numerical_gradient(function_2, np.array([0.0, 2.0])))
print(numerical_gradient(function_2, np.array([3.0, 0.0])))

def gradient_descent(f, init_x, lr= 0.01, step_num=100):
    '''
    f는 최적화려는 함수
    init_x는 초깃값
    lr은 학습률
    step_num은 경사법에 따른 반복 횟수
    함수의 기울기는 앞서 정의한 numerical_gradient로 구하고
    그 기울기에 학습률을 곱한 값으로 갱신하는 처리를 setp_num번 반복
    '''
    x = init_x
    
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x
	
	# 학습률이 너무 큰 경우 lr = 10.0
init_x = np.array([-3.0, 4.0])
gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)

# 학습률이 너무 작은 경우 lr = 1e-10
init_x = np.array([-3.0, 4.0])
gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)



from scratch.common.functions import softmax, cross_entropy_error
from scratch.common.gradient import numerical_gradient

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3)
    
    def predict(self, x):
        return np.dot(x, self.W)
    
    def loss(self,x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)
        
        return loss

net = simpleNet()
print(net.W)

[[-0.98857721 -1.19498403  1.7798275 ]
 [ 1.92945432  2.08198445  0.48024869]]

x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
np.argmax(p)

[ 1.14336256  1.15679559  1.50012032]

2

t = np.array([0,0,1])
net.loss(x, t)

0.87935693550588223

def f(W):
    return net.loss(x, t)

dW = numerical_gradient(f, net.W)
print(dW)

[[ 0.17430645  0.17666371 -0.35097016]
 [ 0.26145968  0.26499557 -0.52645524]]

 

from scratch.common.functions import *
from scratch.common.gradient import numerical_gradient

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
    
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
    
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis = 1)
        t = np.argmax(t, axis = 1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads

net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)
net.params['W1'].shape  # (784, 100)
net.params['b1'].shape  # (100,)
net.params['W2'].shape  # (100, 10)
net.params['b2'].shape  # (10,)

(10,)



x = np.random.rand(100, 784)
t = np.random.rand(100, 10)

%time grads = net.numerical_gradient(x, t)

print(grads['W1'].shape)
print(grads['b1'].shape)
print(grads['W2'].shape)
print(grads['b2'].shape)

CPU times: user 3min 49s, sys: 564 ms, total: 3min 50s
Wall time: 57.7 s
(784, 100)
(100,)
(100, 10)
(10,)

class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
        
    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y
        return out
    
    def backward(self, dout):
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy
		
		class Relu:
    def __init__(self):
        self.mask = None
        
    def forward(self, x):
        '''
        순전파 시 x <= 0 일 때 값을 0으로 치환한다
        기본적으로 x는 numpy 배열임을 가정한다
        '''
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx
		
		class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        
        return out
    
    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
		
		

# Affine 계층

X = np.random.rand(2)    # Input
W = np.random.rand(2, 3) # Weight
B = np.random.rand(3)    # Bias

print(X.shape)
print(W.shape)
print(B.shape)

Y = np.dot(X,W) + B

(2,)
(2, 3)
(3,)

class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
        
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)
        
        return dx


		
		class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None   # Loss
        self.y = None      # Output
        self.t = None      # Target
        
    def foward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
		
		from scratch.common.layers import *
from scratch.common.gradient import numerical_gradient

from collections import OrderedDict

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # initializa Weights
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
        # Build Layers
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        
        self.lastLayer = SoftmaxWithLoss()
    
    
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x
        
    
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1 :
            t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y==t) / float(x.shape[0])
        return accuracy
    
    
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads
    
    def gradient(self, x, t):
        # foward
        self.loss(x, t)
        
        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db
        return grads

from scratch.dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
print('number of iterations is', iters_num)
train_size = x_train.shape[0]
print('train size is', train_size)
batch_size = 100
print('batch size is', batch_size)
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

print('='*20 + '>')
epoch = 0
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.gradient(x_batch, t_batch)

    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    
    if i % iter_per_epoch == 0:
        epoch += 1
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print('Epoch', epoch, ':  ',train_acc,'\t', test_acc)
        

number of iterations is 10000
train size is 60000
batch size is 100
====================>
Epoch 1 :   0.102966666667 	 0.1055
Epoch 2 :   0.904416666667 	 0.9074
Epoch 3 :   0.9235 	 0.9252
Epoch 4 :   0.936166666667 	 0.9344
Epoch 5 :   0.9447 	 0.9432
Epoch 6 :   0.951983333333 	 0.9506
Epoch 7 :   0.955983333333 	 0.9537
Epoch 8 :   0.960733333333 	 0.9562
Epoch 9 :   0.965166666667 	 0.9602
Epoch 10 :   0.967366666667 	 0.9616
Epoch 11 :   0.969633333333 	 0.9652
Epoch 12 :   0.9721 	 0.966
Epoch 13 :   0.973833333333 	 0.9676
Epoch 14 :   0.975983333333 	 0.9686
Epoch 15 :   0.976883333333 	 0.9703
Epoch 16 :   0.9778 	 0.9694
Epoch 17 :   0.97895 	 0.9702

import matplotlib.pyplot as plt
plt.figure(figsize=(20,8))
plt.plot(train_loss_list[:1000], linewidth=0.5)
plt.title('Train Loss Graph')
plt.xlabel('iteration')
plt.ylabel('loss')
plt.show()



plt.figure(figsize=(20,8))
plt.plot(train_acc_list, linewidth=1)
plt.plot(test_acc_list, '-.' ,linewidth=1)
plt.legend(['train acc', 'test acc'], loc=0)
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
    
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
			
			class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
    
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
        
        for key in params.keys():
            self.v[key] = self.momentum * self.v[key]
			
			class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
    
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
			
			

from scratch.dataset.mnist import load_mnist
from scratch.common.util import smooth_curve
from scratch.common.multi_layer_net import MultiLayerNet
from scratch.common.optimizer import *
import matplotlib.pyplot as plt

# 0. MNIST 데이터 읽기==========
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128
max_iterations = 2000


# 1. 실험용 설정==========
optimizers = {}
optimizers['SGD'] = SGD()
optimizers['Momentum'] = Momentum()
optimizers['AdaGrad'] = AdaGrad()
optimizers['Adam'] = Adam()
#optimizers['RMSprop'] = RMSprop()

networks = {}
train_loss = {}
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784, hidden_size_list=[100, 100, 100, 100],
        output_size=10)
    train_loss[key] = []    


# 2. 훈련 시작==========
for i in range(max_iterations):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    for key in optimizers.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        optimizers[key].update(networks[key].params, grads)
    
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)
    
    if i % 100 == 0:
        print( "===========" + "iteration:" + str(i) + "===========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))


# 3. 그래프 그리기==========
markers = {"SGD": "o", "Momentum": "x", "AdaGrad": "s", "Adam": "D"}
x = np.arange(max_iterations)
plt.figure(figsize=(20,8))
for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)
plt.xlabel("iterations")
plt.ylabel("loss")
plt.ylim(0, 1)
plt.legend()
plt.show()

===========iteration:0===========
SGD:2.37670101396
Momentum:2.33816339389
AdaGrad:2.12533763909
Adam:2.15575092649

===========iteration:1900===========
SGD:0.154444681428
Momentum:0.020718003123
AdaGrad:0.0183070478127
Adam:0.0222197490211



import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.random.randn(1000, 100)
node_num= 100
hidden_layer_size = 5
activations ={}

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]
        
    w = np.random.randn(node_num, node_num) / np.sqrt(node_num)
    
    a = np.dot(x, w)
    z = sigmoid(a)
    activations[i] = z
    
fig = plt.figure(figsize=(10,2))
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    plt.yticks([],[])
    plt.hist(a.flatten(), 30, range=(0,1))
plt.show()



class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
    
    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)
    
    def backward(self,dout):
        return dout * self.mask

from scratch.common.multi_layer_net_extend import MultiLayerNetExtend
from scratch.common.trainer import Trainer

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임
x_train = x_train[:300]
t_train = t_train[:300]

# 드롭아웃 사용 유무와 비울 설정 ========================
use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False
dropout_ratio = 0.2
# ====================================================

network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],
                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)
trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=301, mini_batch_size=100,
                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)
trainer.train()

train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list

# 그래프 그리기==========
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

